name: Run Playwright Scraper

# ── TRIGGERS ────────────────────────────────────────────────────────────────
on:
  workflow_dispatch:           # manual trigger
  schedule:                    # three daily runs (UTC)
    - cron: '0 9 * * *'        # 09:00
    - cron: '30 23 * * *'      # 23:30
    - cron: '45 5 * * *'       # 00:30  ── fixed "30 00" → "30 0"

# Abort an older run of the same branch if a new one starts
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: '3.11'
  PLAYWRIGHT_BROWSERS_PATH: /opt/playwright   # browsers cached across jobs

jobs:
  scrape-and-submit:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
    # 1. Checkout
    - uses: actions/checkout@v4

    # 2. Cache pip download dir (+ use setup-python’s built-in “cache: pip”)
    - name: Cache pip
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    # 3. Python
    - uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'                # second-layer wheel cache

    - name: Install Python deps
      run: pip install -r requirements.txt

    # 4. Playwright (browsers are cached in /opt/playwright automatically)
    - name: Install Playwright browsers & deps
      run: python -m playwright install --with-deps chromium

    # 5. Build config.json from Secrets (keeps them masked in logs)
    - name: Build runtime config
      env:
        FORM_URL:        ${{ secrets.FORM_URL }}
        LOGIN_URL:       ${{ secrets.LOGIN_URL }}
        SECRET_KEY:      ${{ secrets.SECRET_KEY }}
        LOGIN_EMAIL:     ${{ secrets.LOGIN_EMAIL }}
        LOGIN_PASSWORD:  ${{ secrets.LOGIN_PASSWORD }}
        OTP_SECRET_KEY:  ${{ secrets.OTP_SECRET_KEY }}
      run: |
        cat > config.json <<'JSON'
        {
          "debug": false,
          "form_url":        "${FORM_URL}",
          "login_url":       "${LOGIN_URL}",
          "secret_key":      "${SECRET_KEY}",
          "login_email":     "${LOGIN_EMAIL}",
          "login_password":  "${LOGIN_PASSWORD}",
          "otp_secret_key":  "${OTP_SECRET_KEY}",
          "initial_concurrency": 8,
          "num_form_submitters": 2,
          "page_timeout_ms": 90000,
          "element_wait_timeout_ms": 20000,
          "auto_concurrency": { "enabled": false }
        }
        JSON

    # 6. Run your scraper
    - name: Run scraper
      run: python scraper.py

    # 7. Persist logs & screenshots for debugging
    - name: Upload artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-output-${{ github.run_id }}
        path: |
          output/
          app.log
          state.json
        retention-days: 7          # auto-purge after a week
